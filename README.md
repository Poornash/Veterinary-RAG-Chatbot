ğŸ¾ PawMedBot â€“ Offline Veterinary AI Assistant (RAG-based)

PawMedBot is an AI-powered veterinary assistant built using Retrieval-Augmented Generation (RAG).
It works offline using a local LLM (via Ollama) and provides reliable, document-grounded answers related to pet health, diseases, care, and veterinary guidance.

This project is developed as part of an AI/ML academic project and is optimized for CPU-only systems, with optional GPU acceleration if available.
ğŸš€ Features

ğŸ¶ Veterinary Q&A (dogs, cats, birds, rabbits, etc.)
ğŸ“š RAG-based answers using uploaded veterinary books & documents
ğŸ§  Local LLM inference (offline support)
âš¡ Fast non-pet query rejection
ğŸ—‚ï¸ Chat history & user authentication
ğŸ–¥ï¸ Clean Streamlit-based UI
ğŸ”’ No external API dependency (privacy-friendly)

ğŸ› ï¸ Tech Stack

Python 3.11
Streamlit â€“ Frontend
LangChain â€“ RAG pipeline
ChromaDB â€“ Vector database
HuggingFace Embeddings â€“ Text embeddings
Ollama â€“ Local LLM runtime
SQLite â€“ User & chat history storage

ğŸ“‚ Project Structure

RAGBOT/
â”œâ”€â”€ app.py
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”œâ”€â”€ auth.py
â”‚   â”œâ”€â”€ db.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 1_login.py
â”‚   â”œâ”€â”€ 2_signup.py
â”‚   â”œâ”€â”€ 3_profile.py
â”‚   â”œâ”€â”€ 4_chat.py
â”‚   â””â”€â”€ 5_history.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ veterinary_books.pdf
â”‚   â”œâ”€â”€ clinics.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ğŸ§© Prerequisites
1ï¸âƒ£ Python (MANDATORY)

Install Python 3.11.x

ğŸ”— Download:
https://www.python.org/downloads/

âœ” During installation:

Check â€œAdd Python to PATHâ€

Verify:
```bash
python --version
```

2ï¸âƒ£ Ollama (Local LLM Runtime)

Install Ollama for your OS:

ğŸ”— https://ollama.com/download

Verify installation:
```bash
ollama --version
```
ollama --version

3ï¸âƒ£ Download Required LLM Model
We use Phi-3 Mini (fast + lightweight).
```bash
ollama pull phi3:mini
```
option if LARGER MODEL WORKS
```bash
ollama pull llama3:instruct
```
ğŸ“¦ Project Setup Instructions
1ï¸âƒ£ Clone the Repository
```basg=h
git clone https://github.com/<your-username>/PawMedBot.git
cd PawMedBot
```

2ï¸âƒ£ Create a Virtual Environment (Recommended)
```bash
python -m venv venv
```
Activate it:
Windows
```
venv\Scripts\activate
```
Mac/Linux
```
source venv/bin/activate
```

3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```

If pip gives errors, upgrade it:
```
python -m pip install --upgrade pip
```

ğŸ“š Build the Vector Database (IMPORTANT)

This step converts the documents in /data into embeddings.
```
python backend/preprocessor.py
```
âœ” This will:
Chunk documents
Generate embeddings
Create vectorstore/ folder automatically
â±ï¸ Takes time depending on system performance (normal).
â–¶ï¸ Run the Application
```
python -m streamlit run app.py
```
Then open the browser at:
http://localhost:8501

ğŸ‘¤ First-Time Usage Flow
Open app
Signup with email & password
Login
Go to Chat
Ask veterinary-related questions
View history & profile anytime

ğŸ§  How the AI Works (High-Level)

Retrieval:
Relevant document chunks are fetched from ChromaDB

Augmentation:
Retrieved context is injected into a prompt

Generation:
Local LLM (Phi-3 Mini) generates an answer grounded in documents

âš¡ Performance Notes
CPU-only systems may take 20â€“40 seconds per response
GPU-enabled systems will be significantly faster
Non-pet queries are rejected instantly (rule-based filter)

âŒ Common Issues & Fixes
â— Ollama model not found
```
ollama list
```
If missing:
```
ollama pull phi3:mini
```
â— Vectorstore missing error
Run:
```
python backend/preprocessor.py
```
â— Streamlit not found
```
pip install streamlit
```
ğŸ‘¤ First-Time Usage Flow

Open app

Signup with email & password

Login

Go to Chat

Ask veterinary-related questions

View history & profile anytime

ğŸ§  How the AI Works (High-Level)

Retrieval:
Relevant document chunks are fetched from ChromaDB

Augmentation:
Retrieved context is injected into a prompt

Generation:
Local LLM (Phi-3 Mini) generates an answer grounded in documents

âš¡ Performance Notes

CPU-only systems may take 20â€“40 seconds per response

GPU-enabled systems will be significantly faster

Non-pet queries are rejected instantly (rule-based filter)

âŒ Common Issues & Fixes
â— Ollama model not found
ollama list


If missing:

ollama pull phi3:mini

â— Vectorstore missing error

Run:

python backend/preprocessor.py

â— Streamlit not found
pip install streamlit


This project falls under:

Artificial Intelligence & Machine Learning (AIML)
specifically Natural Language Processing (NLP) and Information Retrieval (IR).

â¤ï¸ Final Note

If the project runs on one system, it will run identically on another if these steps are followed correctly.

Happy building ğŸ¾âœ¨


